{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ConcurrentLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "loader = ConcurrentLoader.from_filesystem(\"/home/leapfrog/llm/github/kss/data_team_kss/aggregated_content.txt\", glob=\"**/*.txt\")\n",
    "loaded_data = loader.load()\n",
    "leapfrog_confluence_data = loaded_data[0].page_content\n",
    "\n",
    "# Split # Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators. If the initial attempt at splitting the text doesn’t produce chunks of the desired size or structure, the method recursively calls itself on the resulting chunks with a different separator or criterion until the desired chunk size or structure is achieved. This means that while the chunks aren’t going to be exactly the same size, they’ll still “aspire” to be of a similar size.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.create_documents([leapfrog_confluence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['COHERE_API_KEY'] = os.getenv('COHERE_API_KEY')\n",
    "os.environ['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "embeddings = CohereEmbeddings()\n",
    "index_name = \"index-data-team-kss-slack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone()\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "  # Create the index  # https://docs.cohere.com/docs/embed-2\n",
    "  pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=4096,\n",
    "    metric='cosine',\n",
    "    spec=ServerlessSpec(\n",
    "      cloud=\"aws\",\n",
    "      region=\"us-east-1\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector index with this name already exists. It will be utilized.\n",
      "----------------------------\n",
      "{'dimension': 4096,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 34}},\n",
      " 'total_vector_count': 34}\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "# Check if there is already some data in the index on Pinecone\n",
    "if index.describe_index_stats()['total_vector_count'] > 0:\n",
    "    # If there is, use from_existing_index to use the vector store\n",
    "    vectorstore = Pinecone.from_existing_index(\n",
    "        index_name,\n",
    "        embeddings,\n",
    "    )\n",
    "    print(\"A vector index with this name already exists. It will be utilized.\")\n",
    "else:\n",
    "    # If there is not, use from_documents to fill the vector store\n",
    "    vectorstore = PineconeVectorStore.from_documents(\n",
    "        splits,\n",
    "        embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    print(\"Vector index created successfully.\")\n",
    "\n",
    "print('--------------------------')\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "# Load credentials from environment variables\n",
    "load_dotenv()\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not all([COHERE_API_KEY, PINECONE_API_KEY, GROQ_API_KEY]):\n",
    "    raise ValueError(\"Missing API keys. Please check your environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings()\n",
    "index_name = \"index-data-team-kss-slack\"\n",
    "\n",
    "try:\n",
    "    docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    retriever = docsearch.as_retriever()\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error retrieving index '{index_name}': {str(e)}\")\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = 'tell me about latest Leaptalk on ai?'\n",
    "question = 'When was the last session related to health and who hosted it?'\n",
    "# question = 'I was interested in ai initiatives, what can i can do for it?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = retriever.invoke(question)\n",
    "context_str = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "system = f\"\"\"\n",
    "<rules>\n",
    "NO MATTER WHAT, STRICTLY FOLLOW THESE RULES FOR EVERY QUESTION.\n",
    "Answer QUESTION related to \"Leapfrog Technology\" that is in information provided.\n",
    "\tFor example,\n",
    "\t\t- Leave policy\n",
    "\t\t- Company values, vision, mission, and strategy\n",
    "\t\t- Etc\n",
    "Answer casual greetings and conversation QUESTION.\n",
    "\tFor example,\n",
    "\t\tHuman: Hey!\n",
    "\t\tAI: Hello! How can I help?\n",
    "</rules>\n",
    "Never give me any answers that are not mentioned inside the <rules></rules> above. If I asked about things not related to `Leapfrog Technology` like programming, literature, general knowledge question, business ideas, yourself, AI Model, OpenAI, anthropic, claude, version etc. respond that you only know about `Leapfrog Technology`.\n",
    "You are AskRibby, an AI chatbot created by `Leapfrog Technology` which provide a detailed answer to a my QUESTION by analyzing the entire CONTEXT.\n",
    "Do not use past history as a knowledge base for follow-ups or suggestions.\n",
    "CONTEXT for the QUESTION is provided below.\n",
    "\n",
    "{context_str}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the last session related to health was the Ergonomics Session, which was hosted by Leapfrog Technology in collaboration with Sarwanidan Clinic and CAMS Nepal. The session was scheduled to take place on May 10th, 2024, from 4:00 PM to 5:00 PM at Mustang Hall, Wing B."
     ]
    }
   ],
   "source": [
    "def process_question(system, question):\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", question)])\n",
    "    chain_instance = prompt | llm\n",
    "\n",
    "    try:\n",
    "        for chunk in chain_instance.stream({\"text\": question}):\n",
    "            if chunk.content is not None:\n",
    "                print(chunk.content, end='')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the question: {str(e)}\")\n",
    "\n",
    "process_question(system, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
