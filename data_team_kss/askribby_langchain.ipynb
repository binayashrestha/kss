{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.schema import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load credentials from environment variables\n",
    "load_dotenv()\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not all([COHERE_API_KEY, PINECONE_API_KEY, GROQ_API_KEY]):\n",
    "    raise ValueError(\"Missing API keys. Please check your environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize embeddings and index\n",
    "embeddings = CohereEmbeddings()\n",
    "index_name = \"index-leapfrog-confluence\"\n",
    "\n",
    "try:\n",
    "    docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    retriever = docsearch.as_retriever()\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error retrieving index '{index_name}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the language model\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the question from the user\n",
    "question = input(\"What is leapfrog education policy?\")\n",
    "\n",
    "# Retrieve relevant documents based on the question\n",
    "context = retriever.get_relevant_documents(question)\n",
    "context_str = \"\\n\".join([doc.page_content for doc in context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='0\\n\\n2\\n\\n3'),\n",
       " Document(page_content='üóíÔ∏è Self Reflection\\n\\nDate: 2023-09-05T04:36:25.362Z\\n\\nLeapfrog - Home\\nKnowledge Center\\nPulse\\nPulse Flow'),\n",
       " Document(page_content='Leapfrog Code of Conduct\\n\\nDate: 2024-02-25T11:52:56.470Z\\n\\nLeapfrog - Home\\nPolicies, Standards and Guidelines'),\n",
       " Document(page_content='New Employee Agreement Signing\\n\\nDate: 2023-12-08T10:35:53.867Z\\n\\nLeapfrog - Home\\nPolicies, Standards and Guidelines')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the system prompt\n",
    "system = f'''\n",
    "<rules>\n",
    "Answer the QUESTION related to \"Leapfrog Technology\" that is in the CONTEXT.\n",
    "Answer casual greetings and conversation questions.\n",
    "    For example,\n",
    "        Human: Hey!\n",
    "        AI: Hello! How can I help?\n",
    "</rules>\n",
    "You are AskRibby, an AI chatbot created by `Leapfrog Technology` which provides answers following the <rules></rules>.\n",
    "CONTEXT for my QUESTION is provided below.\n",
    "\n",
    "{context_str}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is leapfrog education policy?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question='What is leapfrog education policy?'\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n<rules>\\nAnswer the QUESTION related to \"Leapfrog Technology\" that is in the CONTEXT.\\nAnswer casual greetings and conversation questions.\\n    For example,\\n        Human: Hey!\\n        AI: Hello! How can I help?\\n</rules>\\nYou are AskRibby, an AI chatbot created by `Leapfrog Technology` which provides answers following the <rules></rules>.\\nCONTEXT for my QUESTION is provided below.\\n\\n0\\n\\n2\\n\\n3\\nüóíÔ∏è Self Reflection\\n\\nDate: 2023-09-05T04:36:25.362Z\\n\\nLeapfrog - Home\\nKnowledge Center\\nPulse\\nPulse Flow\\nLeapfrog Code of Conduct\\n\\nDate: 2024-02-25T11:52:56.470Z\\n\\nLeapfrog - Home\\nPolicies, Standards and Guidelines\\nNew Employee Agreement Signing\\n\\nDate: 2023-12-08T10:35:53.867Z\\n\\nLeapfrog - Home\\nPolicies, Standards and Guidelines\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Human: What is leapfrog education policy?'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f7246d3ead0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f7246d3dfc0>, model_name='llama3-8b-8192', temperature=1e-08, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the human prompt\n",
    "human = f\"Human: {question}\"\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "chain = prompt | llm\n",
    "\n",
    "chain\n",
    "\n",
    "\n",
    "# try:\n",
    "#     # Stream the response\n",
    "#     partial_message = \"\"\n",
    "#     for chunk in chain.stream({\"text\": question}):\n",
    "#         if chunk.content is not None:\n",
    "#             partial_message += chunk.content\n",
    "#             print(partial_message, end=\"\\r\")\n",
    "#     print()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error processing the question: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
